{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sfXBcMPOpJ02"
   },
   "source": [
    "# Loading the dataset\n",
    "\n",
    "Let's import the library and load the dataset that we created in the previous task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H69DABm5qMgt"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wV1Qzr-TrYas"
   },
   "outputs": [],
   "source": [
    "# Set some global parameters\n",
    "\n",
    "# Displaying all columns when displaying dataframes\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "# We will work with trigrams \n",
    "ngrams_degree = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yuV6ny4UvZqf"
   },
   "outputs": [],
   "source": [
    "# if there's a problem with the versions of the librairies, you can . . uncomment this line and install the proper versions\n",
    "\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fHoDlqr_qPub"
   },
   "outputs": [],
   "source": [
    "# Load data into pandas dataframe, shuffle it and reset the index\n",
    "df = pd.read_csv('https://alexip-ml.s3.amazonaws.com/stackexchange_812k.tokenized.csv.gz', compression='gzip').sample(frac = 1, random_state = 8).reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TL9EncUpxTN4"
   },
   "source": [
    "The tokens were saved as strings with space separated tokens for simplicity purposes. \n",
    "\n",
    "Let's transform the tokens colum as an actual list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T3eW9r6erVG1"
   },
   "outputs": [],
   "source": [
    "df['tokens'] = df.tokens.apply(lambda txt : txt.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "pLajyrl-sOey",
    "outputId": "89672f76-cf37-45ff-e57d-0625130292a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['my', 'answer', 'is', 'relevant', 'per', \"'\", 's', 'comment', 'which', 'stats', 'tests', 'to', 'use', '?']),\n",
       "       list(['your', 'suggested', 'method', 'doesn', \"'\", 't', 'seem', 'to', 'account', 'for', 'autocorrelation', '-', 'i', '.', 'e', '.', 'values', 'for', 'the', 'response', 'will', 'be', 'more', 'similar', 'the', 'closer', 'together', 'in', 'time', 'the', 'observations', 'are', ',', 'violating', 'any', 'assumptions', 'of', 'independent', 'data', 'points', '.', 'would', 'it', 'be', 'better', 'to', 'e', '.', 'g', '.', 'run', 'multiple', 'arma', 'models', ',', 'each', 'with', 'a', 'different', 'adstock', 'rate', ',', 'then', 'use', 'something', 'like', 'aic', 'to', 'pick', 'the', 'final', 'model', '?']),\n",
       "       list(['incidentally', ',', 'the', 'factorisation', 'theorem', 'argument', 'isn', \"'\", 't', 'mine', 'but', 'a', 'result', 'which', 'goes', 'back', 'to', 'fisher', 'neyman', 'and', 'is', 'in', 'any', 'intro', 'stat', 'inference', 'text', '.']),\n",
       "       list(['your', 'rule', 'of', 'thumb', 'is', 'not', 'particularly', 'good', 'if', '.']),\n",
       "       list(['please', 'edit', 'your', 'question', 'to', 'mention', 'that', 'you', \"'\", 've', 'simultaneously', 'cross', '-', 'posted', 'this', 'question', 'at', 'and', 'if', 'you', 'accept', 'an', 'answer', 'at', 'one', 'forum', ',', 'please', 'mention', 'that', 'at', 'the', 'other', 'forum', 'so', 'folks', 'can', 'spend', 'time', 'on', 'questions', 'that', 'have', 'yet', 'to', 'be', 'answered', '.'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5).tokens.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uqjl-9BxdAWm"
   },
   "source": [
    "We split the dataset into a training and a testing subset. \n",
    "\n",
    "The testing subset is somposed of the titles, the train subset is composed of posts and comments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VYlDBQtadOfU"
   },
   "outputs": [],
   "source": [
    "df_train = df[df.category.isin(['post','comment'])].copy()\n",
    "df_test = df[df.category.isin(['title'])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 799
    },
    "colab_type": "code",
    "id": "lN7UD6Baqr4E",
    "outputId": "ca897b4f-4f07-4a3c-9782-bfeba7026ee2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Training set: (705964, 7)\n",
      "\n",
      "   post_id  parent_id  comment_id  \\\n",
      "0   161009        NaN    309845.0   \n",
      "1   156252        NaN    298634.0   \n",
      "2   423360        NaN    790161.0   \n",
      "3   268623        NaN         NaN   \n",
      "4   433662        NaN    808873.0   \n",
      "\n",
      "                                                text category  \\\n",
      "0  I can't disclose the algorithm, but I can cert...  comment   \n",
      "1  I plan to leave the answer to this question in...  comment   \n",
      "2  Wait, I need to clarify how is Half-normal dis...  comment   \n",
      "3  I am fitting several models of the form.. glm ...     post   \n",
      "4  If you really want to calculate some p-value u...  comment   \n",
      "\n",
      "                                              tokens  n_tokens  \n",
      "0  [i, can, ', t, disclose, the, algorithm, ,, bu...        40  \n",
      "1  [i, plan, to, leave, the, answer, to, this, qu...        84  \n",
      "2  [wait, ,, i, need, to, clarify, how, is, half,...        25  \n",
      "3  [i, am, fitting, several, models, of, the, for...        82  \n",
      "4  [if, you, really, want, to, calculate, some, p...        66  \n",
      "\n",
      "-- Testing set (83685, 7)\n",
      "\n",
      "    post_id  parent_id  comment_id  \\\n",
      "21   154700        NaN         NaN   \n",
      "24   160640        NaN         NaN   \n",
      "27   148203        NaN         NaN   \n",
      "28   327174        NaN         NaN   \n",
      "33   169986        NaN         NaN   \n",
      "\n",
      "                                                 text category  \\\n",
      "21  Are aov with Error same as lmer of lme package...    title   \n",
      "24  How to compare contingency tables for a specif...    title   \n",
      "27        One-sided significance test for correlation    title   \n",
      "28  Visualization activization maximization for re...    title   \n",
      "33  Meaning of Intercept and what the intercept sh...    title   \n",
      "\n",
      "                                               tokens  n_tokens  \n",
      "21  [are, aov, with, error, same, as, lmer, of, lm...        13  \n",
      "24  [how, to, compare, contingency, tables, for, a...        10  \n",
      "27  [one, -, sided, significance, test, for, corre...         7  \n",
      "28  [visualization, activization, maximization, fo...         8  \n",
      "33  [meaning, of, intercept, and, what, the, inter...        14  \n"
     ]
    }
   ],
   "source": [
    "# Display the dimensions of the dataframe \n",
    "print(\"-- Training set: {}\\n\".format(df_train.shape))\n",
    "# and the 1st 5 lines\n",
    "print(df_train.head())\n",
    "\n",
    "print(\"\\n-- Testing set {}\\n\".format(df_test.shape))\n",
    "print(df_test.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oegvQAzqyyST"
   },
   "source": [
    "# Counting bigrams and following tokens\n",
    "We build a counts object defined as a defaultdict(Counter). \n",
    "\n",
    "Taking into account all trigrams (ngrams_degree = 3) that we break into prefix (bigrams) followed by single tokens. \n",
    "\n",
    "The counts object will have the bigrams as keys and for each key a Counter of all the potential tokens. \n",
    "\n",
    "For instance, if the corpus contains a 100 instances of \"*how many people*\" and a 120 instances of \"*how many times*\" we would get the following entry:\n",
    "\n",
    "    counts[('how', 'many')] = Counter('people': 100, 'times': 120, .... )\n",
    "\n",
    "Similarly if the corpus contains \"*the model is*\" 500 times and \"*the model parameters*\" 200 times, we end up with:\n",
    "\n",
    "    counts[('the', 'model')] = Counter('is': 500, 'parameters': 200, .... )\n",
    "\n",
    "To split the tokens into bigramns we use the [ntlk.ngrams](https://www.nltk.org/api/nltk.html#nltk.util.ngrams) function:\n",
    "\n",
    "\n",
    "    Return the ngrams generated from a sequence of items, as an iterator.\n",
    "    For example:\n",
    "\n",
    "    >>> from nltk.util import ngrams\n",
    "    >>> list(ngrams([1,2,3,4,5], 3))\n",
    "    [(1, 2, 3), (2, 3, 4), (3, 4, 5)]\n",
    "\n",
    "The next cell should take a couple of minutes.\n",
    "\n",
    "Note that we build the mode on the training subset df_train and leave the testing subset aside.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "L8cvgLCysPZR",
    "outputId": "ed36d02c-4bee-41bb-d7da-ac9a01f5aa14"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 705964/705964 [02:16<00:00, 5179.64it/s]\n"
     ]
    }
   ],
   "source": [
    "counts = defaultdict(Counter)\n",
    "for tokens in tqdm(df_train.tokens.values):\n",
    "    for ngram in ngrams(\n",
    "          tokens, \n",
    "          n= ngrams_degree,  \n",
    "          pad_right=True, \n",
    "          pad_left=True, \n",
    "          left_pad_symbol=\"<s>\", \n",
    "          right_pad_symbol=\"</s>\"):\n",
    "      \n",
    "        prefix = ngram[:ngrams_degree-1]\n",
    "        token = ngram[ngrams_degree-1]\n",
    "        counts[prefix][token] +=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NIfMPB3H1PZF"
   },
   "source": [
    "We can explore the counts object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TbMNhbnCsb3b",
    "outputId": "4c754da2-edbe-46a3-88be-bc5fef41d91e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 3332935 bigrams\n"
     ]
    }
   ],
   "source": [
    "print(\"we have {} bigrams\".format(len(counts.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "UZAt_buMscsL",
    "outputId": "b49bdc8c-ebf6-48b5-a8d8-856e8755a8dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ecdf', 'output'): \tCounter({'for': 1})\n",
      "('statistic', 'whether'): \tCounter({'my': 1})\n",
      "(',', 'texvars'): \tCounter({',': 1, '.': 1})\n",
      "('act', 'out'): \tCounter({'within': 1, 'with': 1, 'together': 1})\n",
      "('object', 'created'): \tCounter({'by': 9, 'in': 2, 'from': 1})\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for i in range(5):\n",
    "    prefix = random.choice(list(counts.keys()))\n",
    "    print(\"{}: \\t{}\".format(prefix,counts[prefix]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5coZ8qANhBmn"
   },
   "source": [
    "Let's look at the number of potential tokens for each bigram. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QKHGK16sicW_"
   },
   "outputs": [],
   "source": [
    "tokens_count = [ len(v)   for k,v in counts.items()  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "z36-SPk0hLqe",
    "outputId": "d1a108a7-4fae-458d-eac3-836cc5472357"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAucAAAFlCAYAAABIl0aGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/j+DfU86BVI6xwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1,1, figsize=(12,6))\n",
    "plt.hist(tokens_count, bins = 100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mgWVBhFOjTtN"
   },
   "source": [
    "As we can see, most bigrams only have one potential following token. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "QUI1f7ChjXDT",
    "outputId": "9e8848bd-eba3-4aeb-b88c-48de227f35fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2146242 bigrams_with_single_tokens\n",
      "446829 bigrams_with_two_tokens\n"
     ]
    }
   ],
   "source": [
    "bigrams_with_single_tokens = [ k   for k,v in counts.items() if len(v) == 1 ]\n",
    "bigrams_with_two_tokens = [ k   for k,v in counts.items() if len(v) == 2 ]\n",
    "\n",
    "print(\"{} bigrams_with_single_tokens\".format(len(bigrams_with_single_tokens)))\n",
    "print(\"{} bigrams_with_two_tokens\".format(len(bigrams_with_two_tokens)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "66Oqu0fni4xo"
   },
   "source": [
    "2 bigrams have over 10,000 potential tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "maq32UCnhCVf",
    "outputId": "bf23737a-fa01-41a0-d53e-3cb0e1ffc952"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('<s>', '<s>'): 11564, ('of', 'the'): 10815}"
      ]
     },
     "execution_count": 46,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_dict = { k:len(v)   for k,v in counts.items() if len(v) > 10000 }\n",
    "tokens_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gs4sLf7ZkgOU"
   },
   "source": [
    "Note: At this point we could decide to remove all the bigrams with a single potential token as not being significant. This would reduce the size of the model. We will see later on if that actually improves the model or degrades it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a717NY_i3cyK"
   },
   "source": [
    "# token / prefix probabilities\n",
    "\n",
    "To obtain token / prefix probabilities using the Maximum Likelihood Estimator, we must simply normalize each (prefix - token) count by the total number of the prefix occurence. \n",
    "\n",
    "$$p(token / prefix) = \\frac{count(prefix + token)} {count(prefix)}$$\n",
    "\n",
    "\n",
    "Keeping the same defaultdict(Counter) structure for the freq object, we should obtain something similar to \n",
    "\n",
    "\n",
    "    freq[('how', 'many')] = {'people': 0.14, 'times': 120, .... }\n",
    "\n",
    "with \n",
    "* p(people / how many) = c('how many people') / c('how many') \n",
    "* p(times / how many) = c('how many times') / c('how many')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lt1QYcHlssWI"
   },
   "outputs": [],
   "source": [
    "freq = defaultdict(dict)\n",
    "for prefix, tokens in counts.items():\n",
    "    total = sum( counts[prefix].values()  )\n",
    "    for token, c in tokens.items():\n",
    "        freq[prefix][token] = c / total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DdpOwLnC4sVu"
   },
   "source": [
    "Which gives us the following sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "QsSnzS8x4mMe",
    "outputId": "66a3ab1b-3bf2-4221-c89d-da5725a52ec2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('new', 'trainset'): \t{',': 0.5, 'has': 0.5}\n",
      "('model', 'json'): \t{'model': 0.5, 'serialize': 0.5}\n",
      "('intercept', 'det'): \t{'correlation': 1.0}\n",
      "('probabilty', 'equal'): \t{'to': 1.0}\n",
      "('theorem', 'verifying'): \t{'that': 1.0}\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    prefix = random.choice(list(freq.keys()))\n",
    "    print(\"{}: \\t{}\".format(prefix,freq[prefix]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "70ZlWvA54_JF"
   },
   "source": [
    "# Text generation\n",
    "\n",
    "Next we write a text generating function which:\n",
    "* takes a bigram as input\n",
    "  * Note: In this version of the function, the bigram must exist in the corpus\n",
    "* generates a new token by sampling the available tokens related to the bigram using the freq object as distribution \n",
    "* sliding the bigram to include the new token\n",
    "* generating a new token based on the new bigram\n",
    "* stopping when the text is N tokens long or the latest token is the end of string symbol\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OGlrLVja4zC9"
   },
   "outputs": [],
   "source": [
    "def generate(text, n_words = 40):\n",
    "    for i in range(n_words):\n",
    "        prefix = tuple(text.split()[-ngrams_degree+1:])\n",
    "        # no available text\n",
    "        if len(freq[prefix]) == 0:\n",
    "            break\n",
    "        candidates  = list(freq[prefix].keys())\n",
    "        probas      = list(freq[prefix].values())\n",
    "        text       += ' ' + np.random.choice(candidates, p = probas)\n",
    "        if text.endswith('</s>'):\n",
    "            break\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SYqAncrc6K0R"
   },
   "source": [
    "Now let's have some fun with that language model.\n",
    "\n",
    "You can choose any seed bigramns as long as it is present in the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "_JH3qRGG6Gjb",
    "outputId": "4c8cbd56-d2e4-4951-a6d0-87b296c4db42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "the model for every possible single - step random walks and then you could get some non - response curves for individual distribution identification and confidence limits for the comments , why would it just sounds horribly wrong in running mixed design\n",
      "\n",
      "that distribution . also do not necessarily prior experiment , or with margins that is relevant to ordinal , , , , . e - . - . . . . - . - - el - khouri doi . - .\n",
      "\n",
      "to determine the virality of the models you ' re asking something different to the f - m q . recode readings as a function of y . for this . the original data ' data will look like . you are\n"
     ]
    }
   ],
   "source": [
    "text      = 'the model'\n",
    "print()\n",
    "print(generate(text))\n",
    "\n",
    "print()\n",
    "text      = 'that distribution'\n",
    "print(generate(text))\n",
    "\n",
    "print()\n",
    "text      = 'to determine'\n",
    "print(generate(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fyqQm1ZP64T_"
   },
   "source": [
    "# Temperature sampling\n",
    "\n",
    "As you may have noticed, for some bigrams, one particular token may be much more frequent than the others potential tokens. \n",
    "\n",
    "For instance:\n",
    "\n",
    "* ('building', 'machine'): \t{'learning': 0.875, 'classification': 0.125}\n",
    "\n",
    "when generating the next token based on the bigram \"*building machine*\", most of the times the word \"learning\" will be chosen instead of \"classification\".\n",
    "\n",
    "In order to compensate these imbalances and improve the chances of less frequent tokens to be chosen we can sample with temperature.\n",
    "\n",
    "In order to increase the randomness of the next token selection given a prefix, we can flatten the distribution using the temperature $$\\tau$$ to define a new probability distribution as such:\n",
    "\n",
    "$$f_{\\tau}(p_i) = \\frac{ p_i^{\\frac{1}{\\tau}} }{ \\sum_j p_j^{\\frac{1}{\\tau}} }$$\n",
    "\n",
    "See [this post](https://stats.stackexchange.com/questions/255223/the-effect-of-temperature-in-temperature-sampling) for a more in-depth explanation on temperature sampling.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hMC4idKN6Ihe"
   },
   "outputs": [],
   "source": [
    "def generate_temp(text, temperature = 1, n_words=30):\n",
    "    for i in range(n_words):\n",
    "        prefix = tuple(text.split()[-ngrams_degree+1:])\n",
    "        # no available next word\n",
    "        if len(freq[prefix]) == 0:\n",
    "            break\n",
    "        candidates  = list(freq[prefix].keys())\n",
    "        initial_probas = list(freq[prefix].values())\n",
    "        # modify distribution\n",
    "        denom   = sum( [ p ** temperature for p in initial_probas ] )\n",
    "        probas  = [ p ** temperature / denom  for p in initial_probas  ]\n",
    "\n",
    "        text       += ' ' + np.random.choice(candidates, p = probas)\n",
    "        if text.endswith('</s>'):\n",
    "            break\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mWhmCw3s963C"
   },
   "source": [
    "Let's generate some text with different values for the temperature.\n",
    "\n",
    "The higher the temperature, the less chaotic (and shorter) the generated text will end up being."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "nBUTOMX09zIu",
    "outputId": "ce0ecfca-7022-4fef-c87c-6af89ddbb65a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n",
      "the model closely . that watson ’ s measurements and consider y - width is nothing degenerate about the rep put your finger than there will in fact correct because not everybody\n",
      "0.5\n",
      "the model even asymptotically the posterior parameters for both prediction that has calculated tau -. , ncol arima mean metrics such as mi . second model b to d vectors or context\n",
      "1\n",
      "the model at the beginning of one where a is on noise levels . all the monthly intervals . in particular with time series , go - to - one - sided\n",
      "3\n",
      "the model . i have a look at the end . </s>\n",
      "10\n",
      "the model . </s>\n"
     ]
    }
   ],
   "source": [
    "text  = 'the model'\n",
    "# text  = 'to determine'\n",
    "# text  = 'not sure'\n",
    "\n",
    "for tau in [0.01, 0.5, 1, 3, 10]:\n",
    "  print(tau)\n",
    "  print(generate_temp(text, temperature = tau))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jQ4OxyWU_RTG"
   },
   "source": [
    "# Perplexity\n",
    "\n",
    "Let's now implement a way to measure the quality of our model.\n",
    "\n",
    "The idea is to estimate the probability of a test sentence given our model. \n",
    "An uncommon sentence should be less probable than a common one.\n",
    "\n",
    "\n",
    "Notes : \n",
    "  1. At this point the sentence should exist in the corpus. Our model does not know yet how to handle out-of-vocabulary (OOV) bigrams, trigrams or tokens.\n",
    "  2. To avoid the problem of underflow caused by multiplying multiple very small floats, we work in the log space:\n",
    "\n",
    "So instead of calculating perplexity with (case ngrams_degree = 3):\n",
    " \n",
    "$$PP(w_{1},\\cdots, w_N) = ( \\prod_{i = 3}^{N} \\frac{1}{ p(w_i/ w_{i-2}w_{i-1} )} )^{\\frac{1}{N}}$$\n",
    "\n",
    "We compute\n",
    "\n",
    "$$PP(w_{1},\\cdots, w_N) = \\exp [ - \\frac{1}{N} {\\sum_{i = 3}^{N} \\log {p(w_i/ w_{i-2}w_{i-1}} } ) ]$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sBaXy5fJ-SgY"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "\n",
    "def perplexity(sentence):\n",
    "    sentence = tokenizer.tokenize(sentence.lower())\n",
    "    N = len(sentence)\n",
    "    logprob = 0\n",
    "    \n",
    "    for ngram in ngrams(\n",
    "          sentence, \n",
    "          n= ngrams_degree,  \n",
    "          pad_right=True, pad_left=True, \n",
    "          left_pad_symbol=\"<s>\", right_pad_symbol=\"</s>\"):\n",
    "        try:\n",
    "          prefix = ngram[:ngrams_degree-1] \n",
    "          token = ngram[ngrams_degree-1]\n",
    "          logprob += np.log( freq[ prefix ][token]  )\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return np.exp(- logprob / N)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HvFOsEsxAFlI"
   },
   "source": [
    "Let's calculate the perplexity on some sentences.\n",
    "\n",
    "Take the time to see how the perplexity score varies when you . . modify the sentence. For instance compare the perplexity for\n",
    "\n",
    "* *the difference between the two approaches is discussed here.*\n",
    "* *the difference between the two approaches is discussed here*\n",
    "* *the difference between the two approaches*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "fASAO_JzAEbc",
    "outputId": "09abf878-8996-44d3-c5c6-cfaa2aa6c1c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[perplexity 22.63] the difference between the two approaches is discussed here\n",
      "\n",
      "[perplexity 38.12] this question really belongs on a different site\n",
      "\n",
      "[perplexity 72.18] The function may only be linear in the region where the points were taken\n"
     ]
    }
   ],
   "source": [
    "sentence = \"the difference between the two approaches is discussed here\"\n",
    "print(\"[perplexity {:.2f}] {}\".format(perplexity(sentence), sentence))\n",
    "\n",
    "sentence = \"this question really belongs on a different site\"\n",
    "print()\n",
    "print(\"[perplexity {:.2f}] {}\".format(perplexity(sentence), sentence))\n",
    "\n",
    "sentence = \"The function may only be linear in the region where the points were taken\"\n",
    "print()\n",
    "print(\"[perplexity {:.2f}] {}\".format(perplexity(sentence), sentence))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ekhHxEZOC0Ht"
   },
   "source": [
    "# Out of Vocabulary (OOV) \n",
    "\n",
    "The main weakness of our model so far is that it does not know how to handle elements that are not already in the original corpus.\n",
    "\n",
    "Since both when generating text and when calculating perplexity we use the count of the prefix in the corpus, when that prefix is missing, the counts = 0  which causes problems with logs and divisions.\n",
    "\n",
    "To remediate to that problem we can artificially assign a probability (although a very low one) to missing ngrams and tokens.\n",
    "\n",
    "This method is called Laplace smoothing. It relies on calculating the frequency of a token / prefix with:\n",
    "\n",
    "$$ p(token / prefix) = \\frac{ count( prefix + token) + \\delta}{count(prefix) + \\delta \\times |N| }$$\n",
    "\n",
    "\n",
    "Where \n",
    "\n",
    "* N is the total number of prefixes in the model\n",
    "* delta is an arbitrary number \n",
    "\n",
    "When the prefix is missing from the original corpus, the probability of a token / prefix will now be:\n",
    "\n",
    "$$p(token / prefix) = \\frac{1} { | N |}$$\n",
    "\n",
    "Let's implement that perplexity with Laplace Smoothing\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Lbwk0_rqrXa"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bMPyoR4xALs2"
   },
   "outputs": [],
   "source": [
    "def perplexity_laplace(sentence,delta = 1):\n",
    "    sentence = tokenizer.tokenize(sentence.lower())\n",
    "    N = len(sentence)\n",
    "    logprob = 0\n",
    "    for ngram in ngrams(\n",
    "          sentence, \n",
    "          n= ngrams_degree,  \n",
    "          pad_right=True, pad_left=True, \n",
    "          left_pad_symbol=\"<s>\", right_pad_symbol=\"</s>\"):\n",
    "        prefix = ngram[:ngrams_degree-1]\n",
    "        token = ngram[ngrams_degree-1]\n",
    "        if prefix in list(counts.keys()):\n",
    "            total = sum( counts[prefix].values()  )\n",
    "            if token in counts[prefix].keys():\n",
    "                # normal calculation\n",
    "                logprob += np.log( (counts[prefix][token] + delta)/ (total + delta * N ) )\n",
    "            else:\n",
    "                logprob += np.log( ( delta)/ (total + delta * N ) )\n",
    "        else:\n",
    "            logprob += - np.log( N )\n",
    "  \n",
    "    return np.exp(-logprob / N)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "argLcwydEbS2"
   },
   "source": [
    "We can now calculate the perplexity of sentences that were not present in the original corpus. \n",
    "\n",
    "For instance: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "kXQ0559WAv27",
    "outputId": "4c028db3-7011-41c3-8ba6-b2e00e9cfeee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[perplexity 142.66] this model belongs on a different planet\n",
      "[perplexity 35.50] this question really belongs on a different site.\n"
     ]
    }
   ],
   "source": [
    "sentence = \"this model belongs on a different planet\"\n",
    "print(\"[perplexity {:.2f}] {}\".format(perplexity_laplace(sentence, delta = 10), sentence))\n",
    "\n",
    "sentence = \"this question really belongs on a different site.\"\n",
    "print(\"[perplexity {:.2f}] {}\".format(perplexity_laplace(sentence, delta = 10), sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "m6uDCIVAAwqp",
    "outputId": "acb0fb1f-27b5-48de-ef67-c77531f6b209"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[perplexity 319.66] this model belongs on a different planet\n",
      "\n",
      "[perplexity 36.10] this question really belongs on a different site.\n"
     ]
    }
   ],
   "source": [
    "sentence = \"this model belongs on a different planet\"\n",
    "print(\"\\n[perplexity {:.2f}] {}\".format(perplexity_laplace(sentence, delta = 1), sentence))\n",
    "\n",
    "sentence = \"this question really belongs on a different site.\"\n",
    "print(\"\\n[perplexity {:.2f}] {}\".format(perplexity_laplace(sentence, delta = 1), sentence))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k008Cdzt0VH5"
   },
   "source": [
    "# Perplexity on the test corpus and sentence probability\n",
    "\n",
    "How do we calculate the perplexity of a model on a test corpus.\n",
    "\n",
    "Let's say we have *m* sentences in the corpus, the perplexity of the corpus is given by \n",
    "\n",
    "$$ PP(Corpus) = P(S_1, \\cdots, S_m)^{-\\frac{1}{N}} $$\n",
    "\n",
    "We can assume that the sentences are independent\n",
    "\n",
    "$$ PP(Corpus) = (\\prod_{k = 1}^{m}  P(S_k))^{-\\frac{1}{N}} $$\n",
    "\n",
    "Which we calculate in the log space to avoid underflow\n",
    "\n",
    "$$ PP(Corpus) = \\exp ( -\\frac{1}{N} \\sum_{k = 1}^{m}  log(P(S_k)) $$\n",
    "\n",
    "So to calculate the perplexity on a test corpus we need to calculate the probability of each single sentence.\n",
    "\n",
    "The following function calculates the probability of a sentence. \n",
    "\n",
    "Instead of using laplace smoothing to deal with the missing bigrams and tokens, we will simply skip missing elements to make the function faster.\n",
    "Implementing laplace smoothing requires several extra conditions that are taking too much time to run.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S9DMcOnEIAcp"
   },
   "outputs": [],
   "source": [
    "def logproba_sentence(sentence, delta = 1):\n",
    "    sentence = tokenizer.tokenize(sentence.lower())\n",
    "    logprob = 0\n",
    "    for ngram in ngrams(\n",
    "        sentence, n= ngrams_degree,  \n",
    "        pad_right=True, pad_left=True, \n",
    "        left_pad_symbol=\"<s>\", right_pad_symbol=\"</s>\"):\n",
    "        prefix = ngram[:ngrams_degree-1]\n",
    "        token = ngram[ngrams_degree-1]\n",
    "        try:\n",
    "          logprob += np.log( freq[prefix][token] )\n",
    "        except:\n",
    "          pass\n",
    "\n",
    "    return logprob\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xv0NB4Qz3T0j"
   },
   "source": [
    "We can now implement the perplexity for a whole set of sentences\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J2vZmwl5xajz"
   },
   "outputs": [],
   "source": [
    "def corpus_perplexity(corpus):\n",
    "  # start by calculating the total number of tokens in the corpus\n",
    "  all_sentences = ' '.join(corpus)\n",
    "\n",
    "  all_tokens =  tokenizer.tokenize(all_sentences.lower())\n",
    "  N = len(tokens)\n",
    "\n",
    "  logprob = 0\n",
    "  for sentence in tqdm(corpus):\n",
    "    logprob += logproba_sentence(sentence)\n",
    "\n",
    "  return np.exp( - logprob / N)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "d0actWrZxwHV",
    "outputId": "956f0690-c16f-47a4-ab90-5779d4692bc5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 17965.91it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20.43715636173114"
      ]
     },
     "execution_count": 183,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The perplexity of a sample of 1000 titles\n",
    "corpus = df_test.text.sample(1000, random_state = 8).values\n",
    "corpus_perplexity(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "VuYVQ3Kd0NV2",
    "outputId": "64dac499-1c2b-4b8d-9ea7-9fbcac66a087"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83685/83685 [00:04<00:00, 20448.28it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7.62349984746957e+109"
      ]
     },
     "execution_count": 185,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and the perplexity of the whole test corpus\n",
    "corpus_perplexity(df_test.text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IwmD9yanIFEv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LPLMDL Task 2",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
